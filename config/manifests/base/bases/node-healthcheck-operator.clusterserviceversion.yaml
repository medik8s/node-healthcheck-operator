apiVersion: operators.coreos.com/v1alpha1
kind: ClusterServiceVersion
metadata:
  annotations:
    alm-examples: '[]'
    capabilities: Basic Install
    categories: OpenShift Optional
    containerImage: quay.io/medik8s/node-healthcheck-operator:v0.0.1
    createdAt: ""
    description: Detect failed Nodes and trigger remediation with a remediation operator.
    olm.skipRange: '>=0.0.1'
    operatorframework.io/suggested-namespace: openshift-workload-availability
    operatorframework.io/suggested-namespace-template: '{"kind":"Namespace","apiVersion":"v1","metadata":{"name":"openshift-workload-availability","annotations":{"openshift.io/node-selector":""}}}'
    repository: https://github.com/medik8s/node-healthcheck-operator
    support: Medik8s
  name: node-healthcheck-operator.v0.0.0
  namespace: placeholder
spec:
  apiservicedefinitions: {}
  customresourcedefinitions:
    owned:
    - description: NodeHealthCheck is the Schema for the nodehealthchecks API
      displayName: Node Health Check
      kind: NodeHealthCheck
      name: nodehealthchecks.remediation.medik8s.io
      resources:
      - kind: NodeHealthCheck
        name: nodehealthchecks
        version: v1alpha1
      specDescriptors:
      - description: "EscalatingRemediations contain a list of ordered remediation
          templates with a timeout. The remediation templates will be used one after
          another, until the unhealthy node gets healthy within the timeout of the
          currently processed remediation. The order of remediation is defined by
          the \"order\" field of each \"escalatingRemediation\". \n Mutually exclusive
          with RemediationTemplate"
        displayName: Escalating Remediations
        path: escalatingRemediations
      - description: Order defines the order for this remediation. Remediations with
          lower order will be used before remediations with higher order. Remediations
          must not have the same order.
        displayName: Order
        path: escalatingRemediations[0].order
      - description: "RemediationTemplate is a reference to a remediation template
          provided by a remediation provider. \n If a node needs remediation the controller
          will create an object from this template and then it should be picked up
          by a remediation provider."
        displayName: Remediation Template
        path: escalatingRemediations[0].remediationTemplate
      - description: "Timeout defines how long NHC will wait for the node getting
          healthy before the next remediation (if any) will be used. When the last
          remediation times out, the overall remediation is considered as failed.
          As a safeguard for preventing parallel remediations, a minimum of 60s is
          enforced. \n Expects a string of decimal numbers each with optional fraction
          and a unit suffix, eg \"300ms\", \"1.5h\" or \"2h45m\". Valid time units
          are \"ns\", \"us\" (or \"µs\"), \"ms\", \"s\", \"m\", \"h\"."
        displayName: Timeout
        path: escalatingRemediations[0].timeout
      - description: HealthyDelay is the time before NHC would allow a node to be
          healthy again. A negative value means that NHC will never consider the node
          healthy and a manual intervention is expected
        displayName: Healthy Delay
        path: healthyDelay
      - description: Remediation is allowed if no more than "MaxUnhealthy" nodes selected
          by "selector" are not healthy. Expects either a non-negative integer value
          or a percentage value. Percentage values must be positive whole numbers
          and are capped at 100%. 0% is valid and will block all remediation. MaxUnhealthy
          should not be used with remediators that delete nodes (e.g. MachineDeletionRemediation),
          as this breaks the logic for counting healthy and unhealthy nodes. MinHealthy
          and MaxUnhealthy are configuring the same aspect, and they cannot be used
          at the same time.
        displayName: Max Unhealthy
        path: maxUnhealthy
      - description: Remediation is allowed if at least "MinHealthy" nodes selected
          by "selector" are healthy. Expects either a non-negative integer value or
          a percentage value. Percentage values must be positive whole numbers and
          are capped at 100%. 100% is valid and will block all remediation. MinHealthy
          and MaxUnhealthy are configuring the same aspect, and they cannot be used
          at the same time.
        displayName: Min Healthy
        path: minHealthy
      - description: 'PauseRequests will prevent any new remediation to start, while
          in-flight remediations keep running. Each entry is free form, and ideally
          represents the requested party reason for this pausing - i.e: "imaginary-cluster-upgrade-manager-operator"'
        displayName: Pause Requests
        path: pauseRequests
      - description: "RemediationTemplate is a reference to a remediation template
          provided by an infrastructure provider. \n If a node needs remediation the
          controller will create an object from this template and then it should be
          picked up by a remediation provider. \n Mutually exclusive with EscalatingRemediations"
        displayName: Remediation Template
        path: remediationTemplate
      - description: "Label selector to match nodes whose health will be exercised.
          \n Selecting both control-plane and worker nodes in one NHC CR is highly
          discouraged and can result in undesired behaviour. \n Note: mandatory now
          for above reason, but for backwards compatibility existing CRs will continue
          to work with an empty selector, which matches all nodes."
        displayName: Selector
        path: selector
      - description: "StormTerminationDelay introduces a configurable delay after
          storm recovery exit criteria are satisfied (for example, when the number
          of healthy nodes rises above the configured minHealthy constraint). While
          this delay is in effect, NHC remains in storm recovery mode and does not
          create new remediations. Once the delay elapses, storm recovery mode exits
          and normal remediation resumes. \n Expects a string of decimal numbers each
          with optional fraction and a unit suffix, e.g. \"300ms\", \"1.5h\" or \"2h45m\".
          Valid time units are \"ns\", \"us\" (or \"µs\"), \"ms\", \"s\", \"m\", \"h\"."
        displayName: Storm Termination Delay
        path: stormTerminationDelay
      - description: UnhealthyConditions contains a list of the conditions that determine
          whether a node is considered unhealthy.  The conditions are combined in
          a logical OR, i.e. if any of the conditions is met, the node is unhealthy.
        displayName: Unhealthy Conditions
        path: unhealthyConditions
      - description: "Duration of the condition specified when a node is considered
          unhealthy. \n Expects a string of decimal numbers each with optional fraction
          and a unit suffix, eg \"300ms\", \"1.5h\" or \"2h45m\". Valid time units
          are \"ns\", \"us\" (or \"µs\"), \"ms\", \"s\", \"m\", \"h\"."
        displayName: Duration
        path: unhealthyConditions[0].duration
      - description: The condition status in the node's status to watch for. Typically
          False, True or Unknown.
        displayName: Status
        path: unhealthyConditions[0].status
      - description: The condition type in the node's status to watch for.
        displayName: Type
        path: unhealthyConditions[0].type
      statusDescriptors:
      - description: 'Represents the observations of a NodeHealthCheck''s current
          state. Known .status.conditions.type are: "Disabled"'
        displayName: Conditions
        path: conditions
        x-descriptors:
        - urn:alm:descriptor:io.kubernetes.conditions
      - description: HealthyNodes specified the number of healthy nodes observed
        displayName: Healthy Nodes
        path: healthyNodes
      - description: InFlightRemediations records the timestamp when remediation triggered
          per node. Deprecated in favour of UnhealthyNodes.
        displayName: In Flight Remediations
        path: inFlightRemediations
      - description: LastUpdateTime is the last time the status was updated.
        displayName: Last Update Time
        path: lastUpdateTime
      - description: ObservedNodes specified the number of nodes observed by using
          the NHC spec.selector
        displayName: Observed Nodes
        path: observedNodes
      - description: Phase represents the current phase of this Config. Known phases
          are Disabled, Paused, Remediating and Enabled, based on:\n - the status
          of the Disabled condition\n - the value of PauseRequests\n - the value of
          InFlightRemediations
        displayName: Phase
        path: phase
        x-descriptors:
        - urn:alm:descriptor:io.kubernetes.phase
      - description: Reason explains the current phase in more detail.
        displayName: Reason
        path: reason
        x-descriptors:
        - urn:alm:descriptor:io.kubernetes.phase:reason
      - description: StormRecoveryStartTime records when storm recovery mode was activated.
          This field is set when StormRecoveryActive becomes true and helps track
          how long the system has been in storm recovery mode.
        displayName: Storm Recovery Start Time
        path: stormRecoveryStartTime
      - description: StormTerminationStartTime records when storm recovery mode regained
          the minHealthy/maxUnhealthy constraint and the storm is about to end (after
          NodeHealthCheckSpec.StormTerminationDelay has passed).
        displayName: Storm Termination Start Time
        path: stormTerminationStartTime
      - description: UnhealthyNodes tracks currently unhealthy nodes and their remediations.
        displayName: Unhealthy Nodes
        path: unhealthyNodes
      - description: ConditionsHealthyTimestamp is RFC 3339 date and time at which
          the unhealthy conditions didn't match anymore. The remediation CR will be
          deleted at that time, but the node will still be tracked as unhealthy until
          all remediation CRs are actually deleted, when remediators finished cleanup
          and removed their finalizers.
        displayName: Conditions Healthy Timestamp
        path: unhealthyNodes[0].conditionsHealthyTimestamp
      - description: HealthyDelayed notes whether a node should be considered healthy,
          but isn't due to NodeHealthCheckSpec.HealthyDelay configuration.
        displayName: Healthy Delayed
        path: unhealthyNodes[0].healthyDelayed
      - description: Name is the name of the unhealthy node
        displayName: Name
        path: unhealthyNodes[0].name
      - description: Remediations tracks the remediations created for this node
        displayName: Remediations
        path: unhealthyNodes[0].remediations
      - description: Resource is the reference to the remediation CR which was created
        displayName: Resource
        path: unhealthyNodes[0].remediations[0].resource
      - description: Started is the creation time of the remediation CR
        displayName: Started
        path: unhealthyNodes[0].remediations[0].started
      - description: TemplateName is required when using several templates of the
          same kind
        displayName: Template Name
        path: unhealthyNodes[0].remediations[0].templateName
      - description: TimedOut is the time when the remediation timed out. Applicable
          for escalating remediations only.
        displayName: Timed Out
        path: unhealthyNodes[0].remediations[0].timedOut
      version: v1alpha1
  description: |
    ### Introduction
    Hardware is imperfect, and software contains bugs. When node level failures such as kernel hangs or dead NICs
    occur, the work required from the cluster does not decrease - workloads from affected nodes need to be
    restarted somewhere.

    However some workloads, such as RWO volumes and StatefulSets, may require at-most-one semantics.
    Failures affecting these kind of workloads risk data loss and/or corruption if nodes (and the workloads
    running on them) are assumed to be dead whenever we stop hearing from them. For this reason it is important
    to know that the node has reached a safe state before initiating recovery of the workload.

    Unfortunately it is not always practical to require admin intervention in order to confirm the node’s true status.
    In order to automate the recovery of exclusive workloads, we provide operators for failure detection
    and remediation.

    ### Failure detection: Node Health Check operator
    The “Node Health Check” (NHC) operator checks each Node’s set of
    NodeConditions (eg. NotReady) against the criteria and thresholds defined in
    NodeHealthCheck configuration. If the Node is deemed to be in a failed
    state, NHC will initiate recovery by using the SIG Cluster API's “External
    Remediation” API to instantiate the configured remediation template which
    specifies the mechanism/controller to be used.

    ### Failure handling: External remediators
    There are multiple remediators for handling node failure that we recommend:
    - Self Node Remediation (SNR)
    - Fence Agents Remediation (FAR)
    - Machine Deletion Remediation (MDR)

    #### Self Node Remediation (SNR)
    SNR uses watchdog timers and heuristics to ensure nodes enter a safe state
    (no longer hosting workloads) within a known and finite period of time,
    before signaling to the system that all Pods on the failed Node are no longer active
    and can be relocated elsewhere.
    In the case of transient errors, the watchdog’s actions will also result in
    the node rebooting and rejoining the cluster - restoring capacity.

    #### Fence Agents Remediation (FAR)
    FAR uses well-known agents to fence unhealthy nodes, and eventually FAR remediates the nodes.
    The remediation includes rebooting the unhealthy node using a fence agent,
    and then evicting workloads from the unhealthy node.

    #### Machine Deletion Remediation (MDR)
    MDR is limited to OpenShift, and it uses Machine API for reprovisioning unhealthy nodes by deleting their machines.
  displayName: Node Health Check Operator
  icon:
  - base64data: base64EncodedIcon
    mediatype: image/png
  install:
    spec:
      deployments: null
    strategy: ""
  installModes:
  - supported: false
    type: OwnNamespace
  - supported: false
    type: SingleNamespace
  - supported: false
    type: MultiNamespace
  - supported: true
    type: AllNamespaces
  keywords:
  - NHC
  - Self Node Remediation
  - SNR
  - Remediation
  - Fencing
  - medik8s
  - k8s
  links:
  - name: Node Healthcheck Operator
    url: https://medik8s.io
  - name: Source Code
    url: https://github.com/medik8s/node-healthcheck-operator
  maintainers:
  - email: medik8s@googlegroups.com
    name: Medik8s Team
  maturity: alpha
  minKubeVersion: 1.20.0
  provider:
    name: Medik8s
    url: https://github.com/medik8s
  version: 0.0.0
